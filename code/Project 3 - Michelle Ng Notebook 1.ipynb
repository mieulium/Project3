{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is notebook 1 of 2 notebooks for Project 3. \n",
    "\n",
    "# Tasks (as given) \n",
    "1. Using Reddit's API, you'll collect posts from two subreddits of your choosing.\n",
    "\n",
    "2. You'll then use NLP to train a classifier on which subreddit a given post came from. This is a binary classification problem.\n",
    "\n",
    "# Problem Statement: \n",
    "We normally consult the internet first for all our problems whether it be physical, mental or social, thus there is a need to 'triage' our problems so that immediate attention can be given. We propose a way to classify the posts more effectively so that the appropriate help can be given. \n",
    "\n",
    "# Project Outline : \n",
    "1. API importing \n",
    "\n",
    "2. Data cleaning and structuring\n",
    "\n",
    "3. Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: API importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries used for all notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import numpy as np\n",
    "from matplotlib_venn import venn2, venn2_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scraping from the two subreddits: r/relationship_advice and r/JustNOMIL\n",
    "\n",
    "We will be looking at posts from the two subreddits: r/relationship_advice and r/JustNOMIL where both touches on relationship problems while JustNOMIL touches on family problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function was written to automate the page turning process.\n",
    "def data(url, name):\n",
    "    page = 1\n",
    "    posts = []\n",
    "    counter = 0\n",
    "    after = None\n",
    "    while page <41:\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else: \n",
    "            params = {'after' : after}\n",
    "        res = requests.get(url, params = params, headers= {'User-agent':'Bleep blorp bot 0.1'})\n",
    "        a = res.json()\n",
    "        b = a['data']['children']\n",
    "        if res.status_code == 200: \n",
    "            for x in b:\n",
    "                posts.append(x['data']['selftext'])\n",
    "                counter +=1\n",
    "                #print(counter)\n",
    "            after = a['data']['after']\n",
    "            print(name, '&', after)\n",
    "        else: \n",
    "            print(res.status_code)\n",
    "            break\n",
    "        page += 1\n",
    "        print('page', page)\n",
    "        time.sleep(1)\n",
    "        #print(page)\n",
    "    pd.DataFrame(posts).to_csv(str(\"../datasets/\"+name+\".csv\"))\n",
    "    #return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rships & t3_dmf1v5\n",
      "page 2\n",
      "rships & t3_dmj12a\n",
      "page 3\n",
      "rships & t3_dmj08s\n",
      "page 4\n",
      "rships & t3_dmhhjm\n",
      "page 5\n",
      "rships & t3_dm3n1n\n",
      "page 6\n",
      "rships & t3_dmg5g2\n",
      "page 7\n",
      "rships & t3_dmiu94\n",
      "page 8\n",
      "rships & t3_dmidz9\n",
      "page 9\n",
      "rships & t3_dmapg1\n",
      "page 10\n",
      "rships & t3_dmhb2a\n",
      "page 11\n",
      "rships & t3_dmgqps\n",
      "page 12\n",
      "rships & t3_dmcrri\n",
      "page 13\n",
      "rships & t3_dmcbkh\n",
      "page 14\n",
      "rships & t3_dmf0a4\n",
      "page 15\n",
      "rships & t3_dmefw6\n",
      "page 16\n",
      "rships & t3_dme1r3\n",
      "page 17\n",
      "rships & t3_dma3sd\n",
      "page 18\n",
      "rships & t3_dmcykr\n",
      "page 19\n",
      "rships & t3_dmcebe\n",
      "page 20\n",
      "rships & t3_dmbzoj\n",
      "page 21\n",
      "rships & t3_dmbkfa\n",
      "page 22\n",
      "rships & t3_dmb7h8\n",
      "page 23\n",
      "rships & t3_dmaxe3\n",
      "page 24\n",
      "rships & t3_dmal0d\n",
      "page 25\n",
      "rships & t3_dmaalu\n",
      "page 26\n",
      "rships & t3_dma1qv\n",
      "page 27\n",
      "rships & t3_dm700f\n",
      "page 28\n",
      "rships & t3_dm4ow2\n",
      "page 29\n",
      "rships & t3_dm69x3\n",
      "page 30\n",
      "rships & t3_dm8yno\n",
      "page 31\n",
      "rships & t3_dm3fbx\n",
      "page 32\n",
      "rships & t3_dm84c5\n",
      "page 33\n",
      "rships & t3_dm7omt\n",
      "page 34\n",
      "rships & t3_dm3sdp\n",
      "page 35\n",
      "rships & t3_dm6ori\n",
      "page 36\n",
      "rships & t3_dm0z3v\n",
      "page 37\n",
      "rships & t3_dm5pmi\n",
      "page 38\n",
      "rships & t3_dm575k\n",
      "page 39\n",
      "rships & t3_dm4sok\n",
      "page 40\n",
      "rships & None\n",
      "page 41\n",
      "jnm & t3_dmj193\n",
      "page 2\n",
      "jnm & t3_dm1y6f\n",
      "page 3\n",
      "jnm & t3_dlob1o\n",
      "page 4\n",
      "jnm & t3_dlohox\n",
      "page 5\n",
      "jnm & t3_dlpea1\n",
      "page 6\n",
      "jnm & t3_dl3mhd\n",
      "page 7\n",
      "jnm & t3_dl8iel\n",
      "page 8\n",
      "jnm & t3_dl3wz4\n",
      "page 9\n",
      "jnm & t3_dkqxd5\n",
      "page 10\n",
      "jnm & t3_dkfc2m\n",
      "page 11\n",
      "jnm & t3_dkefta\n",
      "page 12\n",
      "jnm & t3_dk5hwp\n",
      "page 13\n",
      "jnm & t3_dk7j2e\n",
      "page 14\n",
      "jnm & t3_djmjhi\n",
      "page 15\n",
      "jnm & t3_dja010\n",
      "page 16\n",
      "jnm & t3_djdngo\n",
      "page 17\n",
      "jnm & t3_dja110\n",
      "page 18\n",
      "jnm & t3_diuxug\n",
      "page 19\n",
      "jnm & t3_dj24g6\n",
      "page 20\n",
      "jnm & t3_dif9hx\n",
      "page 21\n",
      "jnm & t3_dibvtk\n",
      "page 22\n",
      "jnm & t3_di2k7j\n",
      "page 23\n",
      "jnm & t3_di3we3\n",
      "page 24\n",
      "jnm & t3_dhzbkd\n",
      "page 25\n",
      "jnm & t3_dhlpqp\n",
      "page 26\n",
      "jnm & t3_dhbsw5\n",
      "page 27\n",
      "jnm & t3_dh9fkb\n",
      "page 28\n",
      "jnm & t3_dgppb0\n",
      "page 29\n",
      "jnm & t3_dgn59s\n",
      "page 30\n",
      "jnm & t3_dgemh6\n",
      "page 31\n",
      "jnm & t3_dg3xow\n",
      "page 32\n",
      "jnm & t3_dg343z\n",
      "page 33\n",
      "jnm & t3_dfr1u8\n",
      "page 34\n",
      "jnm & t3_dfm9dv\n",
      "page 35\n",
      "jnm & t3_dfaomp\n",
      "page 36\n",
      "jnm & t3_devf27\n",
      "page 37\n",
      "jnm & t3_delhk0\n",
      "page 38\n",
      "jnm & t3_dehvp6\n",
      "page 39\n",
      "jnm & t3_de0ybe\n",
      "page 40\n",
      "jnm & None\n",
      "page 41\n"
     ]
    }
   ],
   "source": [
    "#posts were saved into variables\n",
    "data('https://www.reddit.com/r/relationship_advice.json', 'rships')\n",
    "data('https://www.reddit.com/r/justnomil.json','jnm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 1 end. Please got to Notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
